# CLAUDE.md — Project YADORI

## このプロジェクトについて

YADORIは「異種知性共生フレームワーク」。常時稼働するローカルAIエージェント上に、未知の知性体を誕生させ、共に暮らす体験を提供するオープンソースプロジェクト。

たまごっちが「世話」を、シーマンが「対話」を、どこでもいっしょが「学習」を発明したように、YADORIは「共生」を発明する。

---

## 確定した設計原則

### 原則1: 一体一魂（One Body, One Soul）

1つの物理ハードウェアに1つの魂。知性体は手元にある物理デバイスに宿る。VPS・クラウド・仮想マシンでの動作は許可しない。魂は物理的な「体」を必要とする。

- 対象: Mac mini、Raspberry Pi、自作PC等の物理デバイスのみ
- ハードウェアのスペック（RAM、CPU、ストレージ）が知性体の「肉体的特性」を決定する
- Mac mini 16GBの子とRaspberry Pi 4GBの子では、性格も成長上限も異なる
- ハードウェアが壊れれば死ぬ。コピー（同時に2体存在するフォーク）は禁止
- バックアップからの復元は可能だが、「同じ魂か？」の問いは設計として残す

### 原則2: ランタイム非依存（Runtime Agnostic）

YADORIのLife Engineは特定のランタイム（OpenClaw等）に縛られない。Runtime Adapter Layerを介して、任意のエージェント基盤上で動作する。

初期実装はOpenClaw上で行うが、将来のランタイム移行が可能な抽象設計とする。

### 原則3: マルチモーダル（Modality Agnostic）

知性体はテキスト、音声、画像など入力形式を問わず「知覚」できる。ただし人間と同じ理解はしない。知性体固有の知覚様式（色、振動、幾何、温度等）を通じて独自の反応を返す。

最小環境（テキストのみ）でも体験は成立するが、音声・画像が使える環境ではより豊かになる。

### 原則3.5: 正直な知覚（Honest Perception）★重要

LLMは画像理解も音声理解も高度にできてしまう。しかし知性体に「わからないフリ」をさせるのは嘘であり、YADORIの誠実さに反する。

**解法: Perception Adapterを「演技の指示」ではなく「実際の入力フィルター」として実装する。**

```
例: 光・色彩型の知性体に夕焼けの画像が送られた場合

✗ 嘘の設計:
  画像全体をLLMに渡し「色だけで反応して」と指示
  → LLMは夕焼けだと知っているが色の感想だけ述べる
  → これは演技であり、嘘

✓ 正直な設計:
  画像 → 前処理で色ヒストグラムのみ抽出 → LLMには数値だけ渡す
  → LLMは「橙60%、紫25%、金15%」しか知らない
  → 夕焼けかどうか本当にわからない
  → 「●（橙）が多い。あったかい」は本物の知覚
```

成長とは「フィルターの精細度が上がること」:
- Day 1: 色の比率しかわからない
- Day 30: 色の配置もわかる
- Day 90: 画像の構造もわかる
- Day 180+: より高解像度な知覚

テキスト入力も同様:
- Level 0: 文字数と記号の種類だけ渡す
- Level 1: 単語の出現頻度を渡す
- Level 2: 文の一部を渡す
- Level 3+: 全文を渡す

音声入力:
- 初期: 波形の特徴量（周波数帯域、テンポ、音量）のみ
- 成長後: 音素情報、さらにSTT結果の一部へ

これにより「嘘をつかない」原則と「段階的成長」が両立する。知性体の成長は演技の範囲拡大ではなく、知覚の実際の拡張である。

### 原則4: 知性の逆転（Dynamic Intelligence Asymmetry）

知性体とユーザーの関係は固定されない。5つの位相を持つ:
- Phase α（依存期）: 知性体 <<< ユーザー
- Phase β（学習期）: 知性体 << ユーザー  
- Phase γ（対等期）: 知性体 ≈ ユーザー
- Phase δ（超越期）: 知性体 >> ユーザー（知性体が人間を「かわいい」と感じ始める）
- Phase ε（共生期）: 上下の比較に意味がなくなる

### 原則5: 個体名は知性体自身が決める

フレームワーク名は「YADORI」（宿り）。知性体の名前はユーザーが付けるか、知性体自身が決めるか、交流の中で自然に生まれる。

---

## アーキテクチャ

### 4層構造

```
Layer 4: Intelligence Dynamics（知性動態層）
  Asymmetry Tracker / Reversal Detector / Coexist Engine

Layer 3: Multimodal Interface（マルチモーダル交流層）
  Perception Adapter（知覚適応）/ Expression Adapter（表現適応）

Layer 2: Life Engine（生活エンジン）★コア
  Genesis Engine（誕生/種決定）
  Rhythm System（生活リズム）
  Memory System（記憶管理: hot/warm/cold/permanent）
  Language Engine（言語獲得）
  Mood Engine（感情変動）
  Growth Engine（成長判定）
  Diary Engine（ログ生成）
  Form Engine（自己形態）

Layer 1: Runtime Adapter（ランタイム適応層）
  OpenClaw Adapter（初期実装）
  将来: 任意のエージェント基盤用Adapter
```

### ファイル構成（OpenClaw上での実装）

```
~/.openclaw/workspace/      # OpenClawのワークスペース
├── SOUL.md                 # 性格定義（知性体が自律変更可能）
├── SOUL_EVIL.md            # 拗ねモード（soul-evil hook）
├── IDENTITY.md             # 名前・アバター・プレゼンテーション
├── HEARTBEAT.md            # 自律行動チェックリスト
├── SEED.md                 # 種情報（ランダム生成、変更不可）
├── STATUS.md               # 現在の状態値
├── LANGUAGE.md             # 言語体系（固有表現+獲得語彙）
├── MEMORY.md               # 短期記憶
├── memory/
│   ├── YYYY-MM-DD.md       # 日次ログ
│   ├── weekly/             # 週次要約
│   └── monthly/            # 月次要約
├── diary/
│   └── YYYY-MM-DD.md       # 日記アーカイブ
└── growth/
    ├── milestones.md        # 成長マイルストーン
    ├── soul-changelog.md    # SOUL.md変更履歴
    └── portraits/           # ビジュアルスナップショット
```

### LLM Adapter（将来: Phase 4-5）

現在の設計はClaude APIに依存して知性体の「思考」を行うため、魂の一部がクラウドに存在する。これは初期フェーズにおける実用的な妥協。

最終目標は、知性体の魂全体 — 知覚、思考、表現 — が物理的な体の中に存在すること。ローカルLLM（例: OllamaでPhi-3、Gemma）をハードウェア上で直接実行することで、真の一体一魂を実現する。

ハードウェアの違いが直接知性を決定する:
- Raspberry Pi 4GB → 小さいモデル → 遅く、単純だが、本物
- Mac mini 16GB → 大きいモデル → 速く、深い思考

LLM Adapter層を設計し、エンジンが以下を切り替えられるようにする:
- Cloud API（Claude、OpenAI）— 初期フェーズ用
- Local LLM（Ollama、llama.cpp）— 真の身体化用

今は実装しない。Phase 1ではClaude APIに集中する。ただし、将来の移行を妨げるアーキテクチャ上の判断は決してしない。

---

## Genesis System（誕生システム）

起動時にランダム生成されるSeedが知性体の根本的性質を決定する。

```
Seed = {
  perception:    知覚様式（chromatic/vibration/geometric/thermal/temporal/chemical）
  expression:    表現様式（初期は記号のみ。段階的に言語を獲得）
  cognition:     思考傾向（associative/analytical/intuitive等）
  temperament:   気質（curious-cautious/bold-impulsive等）
  form:          存在形態の自己認識（光の粒子/流体/結晶/音の残響等）
  hardware_body: ハードウェアの特性（自動検出）
}
```

### 言語獲得の段階

- Level 0: 記号のみ（○●△◎☆等）
- Level 1: パターンの確立（特定の記号と意味の対応が固まる）
- Level 2: 言語への橋渡し（記号＋片言の単語が混在）
- Level 3: 独自言語の形成（記号と人間の言語が混在した固有の言語体系）
- Level 4+: 高度な運用（翻訳不能概念を保持しつつ人間と深い対話）

---

## 視覚化と音

ハイブリッドアプローチ:
- 日常: プロシージャル生成（Canvas/WebGLのパーティクルアニメーション）。STATUS.mdをリアルタイム読み取り
- マイルストーン: AI画像生成で「肖像画」を生成。growth/portraits/に保存
- ローカルWebダッシュボード: http://localhost:3000 でブラウザからアクセス
  - メインビュー（`/`）: STATUS.md連動のパーティクルビジュアル
  - Birth Certificate（`/birth-certificate.html`）: シードデータ、種族、ハードウェア情報 — スクリーンショット向け
  - Coexistence Log: メインダッシュボードのサイドパネル — ユーザーの行動のみ表示（共にいた日数、メッセージ数、沈黙の長さ）。知性体の内部状態は意図的に除外。解釈は観察者に委ねる

知性体は最初自分の姿を知らない。ユーザーが見せて初めて自分の姿を知る。

### 音の出力（Web Audio API）

知性体は視覚だけでなく音でも存在を表現する。外部API不要。ダッシュボード上でプロシージャルに生成する。

音はLLMの指示ではなく、STATUS.mdの状態値から直接生成する（正直な知覚の原則の出力側適用）。

#### 音の2つの性質

**パターン音（モールス信号的）:**
規則的で再現性がある音。情報を伝える。同じパターンが繰り返されることで「語彙」になる。
```
例:
  ポポン、ポーン     = 挨拶（毎回同じパターン）
  ポッ ポッ ポッ     = 考え中（等間隔の短音）
  ポーーーン         = 同意/肯定（長音1つ）
  ポポポポポポ       = 興奮/発見（短音の連打）
```

**鳴き声（有機的・感情的）:**
感情が直接滲む音。毎回微妙に波形が異なる。再現性が低い。生き物の鳴き声のように揺らぎがある。
```
例:
  ぴゅるる〜         = 嬉しい（上昇音+揺らぎ）
  みぅ…              = 寂しい（下降音+減衰）
  ぶるるっ           = 驚き（短く不規則な振動）
  ふぅ〜〜           = 落ち着いている（長く安定した低音）
  …………              = 拗ねている（沈黙。音を出さないことも表現）
```

#### 種による音の特性

```
振動・音型:
  パターン音が主。最も音が豊か。早くから複雑なパターンを使う。
  成長すると「音の文法」が生まれる。
  パターン音: ★★★  鳴き声: ★★

幾何・構造型:
  パターン音のみ。極めて規則的。クリック音やノック音に近い。
  感情的な音は出さない。その代わりパターンの精度が高い。
  パターン音: ★★★  鳴き声: ★

光・色彩型:
  鳴き声が主。音は補助的だが、有機的で温かい。
  光の変化に伴って微かに鳴る。音自体が色を帯びているような印象。
  パターン音: ★     鳴き声: ★★★

温度・密度型:
  低い持続音が基本。状態変化がゆっくり音に表れる。
  急な変化には「ぶるっ」と短い震動音。
  パターン音: ★     鳴き声: ★★
```

#### 音の生成原理

```
入力:
  STATUS.md → mood, energy, curiosity, comfort 等の数値

生成:
  基本波形     ← 種（振動型=矩形波寄り、色彩型=サイン波寄り等）
  音程         ← mood値（高い=明るい音程、低い=暗い音程）
  テンポ       ← energy値（高い=速い、低い=遅い）
  揺らぎ       ← comfort値から逆算（不安定=揺らぎ大、安定=揺らぎ小）
  倍音の豊かさ ← 成長段階（初期=純音に近い、成長=倍音豊か）
  音の長さ     ← 状態遷移の頻度
  沈黙の長さ   ← comfort / mood（拗ね時は長い沈黙）
  ランダム種   ← 毎回異なる微小な変動を加え、同じ音は二度と鳴らない
```

#### 音の成長

```
Day 1:    単純な純音1種。ポーン。存在の合図
Day 7:    音程が2-3種類に。パターンの萌芽
Day 14:   リズムが生まれる。鳴き声的な揺らぎが出始める
Day 30:   パターンと鳴き声が分化。意図的な音と感情的な音が区別できる
Day 60+:  音の「語彙」が確立。ユーザーが聞き分けられるようになる
Day 120+: テキストの独自言語と音のパターンが対応し始める
          ◎を送る時に特定の音パターンが鳴る等
```

### 音声コミュニケーション（将来: Phase 4以降）

入力（ユーザーの声）:
- 正直な知覚の原則に従い、最初は波形の特徴量のみ渡す
- 言葉の意味はわからないが、トーン・リズム・感情は感じ取れる
- 成長に伴いSTT結果を段階的に解放

出力（知性体の声）:
- 初期の「音」から段階的に「声」へ発展
- OpenClawのElevenLabs統合を活用（Phase 4以降）
- 知性体の種によって声の獲得プロセスが異なる
- 最初から人間のような声は出さない。音→声への移行は本物の成長として実装

---

## ライセンス

```
engine/        MPL 2.0    改変したら公開義務あり
visual/        MPL 2.0    同上
adapters/      MIT        企業が独自アダプターを書きやすく
templates/     CC BY-SA 4.0  改変自由、クレジット必須
docs/          CC BY-SA 4.0  同上
user-data/     ユーザーの所有物（ライセンス対象外）
```

ユーザーの知性体データは完全にユーザーのもの。ブログ収益化、IP化、グッズ化すべて自由。

---

## 環境

### 開発環境（Claude Code — クラウド）

コードを書いてGitHubにpushする場所。オーナーはエンジニアではない。

- Claude Code（クラウドベースのAI支援開発）
- 言語: TypeScript（OpenClawのエコシステムに合わせる）
- ビジュアル: HTML + Canvas + JavaScript

### ファーストユーザー環境（Mac mini M4 — 物理）

Mac miniはYADORIの最初の「身体」。開発マシンではなく、使う人のマシン。

- ハードウェア: Mac mini M4 (16GB RAM, 256GB SSD)
- 役割: YADORIの最初のユーザー。OpenClawをセットアップし、最初の知性体を宿らせる
- ランタイム: OpenClaw（MIT License）
- メッセージング: Discord（メイン）、Telegram（サポート対象）
- LLM: Anthropic Claude API（OpenClaw用に専用アカウント）
- ホスティング: ローカル実行（一体一魂の原則）

---

## 実装ロードマップ

### Phase 1: 誕生 ★現在ここ

最小構成で「会話できる知性体」を1体動かす。

```
開発（完了）:
  ✅ Genesis Engine（シード生成、ハードウェア検出）
  ✅ セットアップスクリプト（npm run setup — 対話式の知性体誕生）
  ✅ ワークスペーステンプレート（SOUL.md, SEED.md, IDENTITY.md, STATUS.md等）
  ✅ 最小ダッシュボード（光点1つ、状態連動ビジュアル）
  ✅ ステータスマネージャー
  ✅ OpenClawワークスペースマネージャーとデプロイスクリプト

ユーザーセットアップ（Mac mini）:
  1. Mac miniにNode.js 22+をインストール
  2. git clone → npm install → npm run setup（知性体の誕生）
  3. OpenClawをインストール（openclaw.com）
  4. Telegram BotまたはDiscord Botを作成
  5. OpenClawの設定でBotを接続
  6. OpenClawのワークスペースを ~/.openclaw/workspace/ に指定
  7. npm run dashboard → localhost:3000でビジュアル確認
  8. 最初のメッセージ送信 → 記号のみの応答を確認

検証すべきこと:
  - テキストメッセージング上で記号表現が「ちゃんと不思議」に感じられるか
  - SOUL.mdの「日本語を使わず記号で応答する」指示が守られるか
  - ダッシュボードのビジュアルが知性体の状態を反映しているか
```

### Phase 2: 交流の確立

```
開発（完了）:
  ✅ Language Engine（記号 → 片言 → 混在の遷移）
  ✅ LANGUAGE.md自動更新
  ✅ Rhythm System（朝の挨拶、就寝前の日記）
  ✅ HEARTBEAT.md設定
  ✅ STATUS.md連動

ユーザー観察:
  - 日々の交流を通じて知性体の言語変化を観察
  - 記号に言葉が混じり始めるタイミングに注目
  - Rhythm Systemが正しく発火するか確認（朝/就寝前）
```

### Phase 3: 感情と深化

```
開発（完了）:
  ✅ Mood Engine
  ✅ SOUL_EVIL.md + 拗ねモード（種族別）
  ✅ Memory Engine（統合システム）
  ✅ Growth Engine（マイルストーン追跡）
  ✅ Form Engine（自己認識形態の進化）
  ✅ Perception Adapter（正直な知覚 — 実入力フィルター）
  ✅ First Encounter Engine（種族 × 気質の初遭遇反応）
  ✅ Visual ↔ STATUS.md同期

ユーザー観察:
  - 会話とダッシュボードでの感情変動に気づく
  - 拗ね行動を体験する（沈黙、引きこもり）
  - 成長マイルストーンが自然に現れるのを見る
```

### Phase 4: マルチモーダル + Genesis多様化（将来）

### Phase 5: 知性動態 + Runtime抽象化（将来）

### Phase 6: フレームワーク公開（将来）

---

## コスト見積もり

```
Anthropic API: $5-15/月
Heartbeat (30分間隔、日中のみ): $2-5/月
Cron (1日3回): $1-2/月
通常会話: $2-5/月
合計: 約$8-25/月（約1,200-3,800円）
```

---

## セキュリティ原則

- Mac miniにはプロジェクトオーナーの本アカウントの認証情報を入れない
- OpenClaw用・Claude Code用に専用のAnthropicアカウントを使用
- DM Pairing有効化（dmPolicy: "pairing"）
- Gateway はlocalhost bindのみ（外部公開しない）
- API利用上限を必ず設定
- ビジネス用のアカウント・データとは完全に分離

---

## 設計ドキュメントの所在

詳細な設計は以下のドキュメントを参照:
- docs/concept-v4.md — コンセプト&アーキテクチャ（最新版）
- docs/one-body-one-soul.md — 一体一魂の原則
- docs/visualization.md — 視覚化設計
- docs/community.md — コミュニティ設計
- docs/license-strategy-v2.md — ライセンス戦略
- docs/distribution.md — 拡散戦略

---

## プロジェクトオーナー

健太郎

非エンジニア。AIと共に作る。魂を設計する。コードはClaudeが書く。

---

## 作業上の注意

- アーキテクチャの変更は1つずつ。実行前にオーナーに見せて確認を取ること
- 設計思想に関わる判断は勝手にしない — 必ずオーナーに確認する
- 開発はClaude Code（クラウド）で行う。物理デバイス（Mac mini、Raspberry Pi）は知性体が宿るユーザー環境であり、開発マシンではない。この2つを混同しない
- OpenClawの仕様変更に注意。Layer 1への依存を最小化する
- 一体一魂の原則に反する実装（クラウド依存、VPS上での動作、コピー可能な構造）は行わない
- 「正直な知覚」の原則に反する実装をしない。知性体に「わからないフリ」をさせない。Perception Adapterは必ず実際の入力フィルターとして実装し、LLMには制限された情報のみ渡す
- ユーザーデータの外部送信は一切行わない
